{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感谢大家来我的网站学习https://www.captainbed.net\n",
    "\n",
    "本次实战我们构建一个深度神经网络，并用它来完成和《第一个人工智能程序》时同样的任务——识别图片中是否有猫。在《第一个人工智能程序》时，我们使用的是单神经元神经网络，本次我们使用深度神经网络来提升预测精准度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\InstalledFiles\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载我们自定义的工具函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些画图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将构建深度神经网络所需的工具函数一个个给编写好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数用于初始化所有层的参数w和b\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n",
    "    例如，layer_dims=[5,4,3]，表示输入层有5个神经元，第一层有4个，最后二层有3个神经元\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n",
    "    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # 获取神经网络总共有几层\n",
    "\n",
    "    # 遍历每一层，为每一层的W和b进行初始化\n",
    "    for l in range(1, L):\n",
    "        # 构建并随机初始化该层的W。由我前面的文章《1.4.3 核对矩阵的维度》可知，Wl的维度是(n[l] , n[l-1]) \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n",
    "        # 构建并初始化b\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # 核对一下W和b的维度是我们预期的维度\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    #就是利用上面的循环，我们就可以为任意层数的神经网络进行参数初始化，只要我们提供每一层的神经元个数就可以了。       \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始构建前向传播所需的工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_forward用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，这个称之为线性前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):   \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_forward用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数，使用了激活函数之后上面的线性前向传播就变成了非线性前向传播了。在dnn_utils.py中我们自定义了两个激活函数，sigmoid和relu。它们都会根据传入的Z计算出A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\": # 如果该层使用sigmoid        \n",
    "        A = sigmoid(Z) \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数构建了一个完整的前向传播过程。这个前向传播一共有L层，前面的L-1层用的激活函数是relu，最后一层使用sigmoid。\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 这个list列表里面包含了每一层的参数w和b\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    # 获取参数列表的长度，这个长度的一半就是神经网络的层数。\n",
    "    # 为什么是一半呢？因为列表是这样的[w1,b1,w2,b2...wl,bl],里面的w1和b1代表了一层\n",
    "    L = len(parameters) // 2  \n",
    "    \n",
    "    # 循环L-1次，即进行L-1步前向传播，每一步使用的激活函数都是relu\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)# 把一些变量数据保存起来，以便后面的反向传播使用\n",
    "        \n",
    "    \n",
    "    # 进行最后一层的前向传播，这一层的激活函数是sigmoid。得出的AL就是y'预测值\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面已经完成了前向传播了。下面这个函数用于计算成本（单个样本时是损失，多个样本时是成本）。\n",
    "# 通过每次训练的成本我们就可以知道当前神经网络学习的程度好坏。\n",
    "def compute_cost(AL, Y):\n",
    "       \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)# 确保cost是一个数值而不是一个数组的形式\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面已经实现了前向传播和成本函数，下面开始实现反向传播。通过反向传播来计算梯度——计算每层的w和b相当于成本函数的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_backward函数用于根据后一层的dZ来计算前面一层的dW，db和dA。也就是实现了下面3个公式\n",
    "$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dZ -- 后面一层的dZ\n",
    "    cache -- 前向传播时我们保存下来的关于本层的一些变量\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_backward用于根据本层的dA计算出本层的dZ。就是实现了下面的公式\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "上式的g'()表示求Z相当于本层的激活函数的偏导数。所以不同的激活函数也有不同的求导公式。\n",
    "我们为大家编写了两个求导函数sigmoid_backward和relu_backward。大家当前不需要关心这两个函数的内部实现，当然，如果你感兴趣可以到dnn_utils.py里面去看它们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dA -- 本层的dA \n",
    "    cache -- 前向传播时保存的本层的相关变量\n",
    "    activation -- 指示该层使用的是什么激活函数: \"sigmoid\" 或 \"relu\"\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    # 这里我们又顺带根据本层的dZ算出本层的dW和db以及前一层的dA\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面这个函数构建出整个反向传播。\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    AL -- 最后一层的A，也就是y'，预测出的标签\n",
    "    Y -- 真实标签\n",
    "    caches -- 前向传播时保存的每一层的相关变量，用于辅助计算反向传播\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # 获取神经网络层数。caches列表的长度就等于神经网络的层数\n",
    "    Y = Y.reshape(AL.shape) # 让真实标签的维度和预测标签的维度一致\n",
    "    \n",
    "    # 计算出最后一层的dA，前面文章我们以及解释过，最后一层的dA与前面各层的dA的计算公式不同，\n",
    "    # 因为最后一个A是直接作为参数传递到成本函数的，所以不需要链式法则而直接就可以求dA（A相当于成本函数的偏导数）\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # 计算最后一层的dW和db，因为最后一层使用的激活函数是sigmoid\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "                                                                                            dAL, \n",
    "                                                                                            current_cache,\n",
    "                                                                                            activation = \"sigmoid\")\n",
    "\n",
    "    # 计算前面L-1层到第一层的每层的梯度，这些层都使用relu激活函数\n",
    "    for c in reversed(range(1,L)): # reversed(range(1,L))的结果是L-1,L-2...1。是不包括L的。第0层是输入层，不必计算。\n",
    "        # 这里的c表示当前层\n",
    "        grads[\"dA\" + str(c-1)], grads[\"dW\" + str(c)], grads[\"db\" + str(c)] = linear_activation_backward(\n",
    "            grads[\"dA\" + str(c)], \n",
    "            caches[c-1],\n",
    "            # 这里我们也是需要当前层的caches，但是为什么是c-1呢？因为grads是字典，我们从1开始计数，而caches是列表，\n",
    "            # 是从0开始计数。所以c-1就代表了c层的caches。数组的索引很容易引起莫名其妙的问题，大家编程时一定要留意。\n",
    "            activation = \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的反向传播，我们得到了每一层的梯度（每一层w和b相当于成本函数的偏导数）。下面的update_parameters函数将利用这些梯度来更新/优化每一层的w和b，也就是进行梯度下降。\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- 每一层的参数w和b \n",
    "    grads -- 每一层的梯度\n",
    "    learning_rate -- 是学习率，学习步进\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # 获取层数。//除法可以得到整数\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print (\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print (\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print (\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，工具函数都编写好了，下面我们开始使用它们来构建一个深度神经网络以识别图片中是否有猫。\n",
    "首先，我们还是老规矩——第一步加载数据集。本次的数据集和《第一个人工智能程序》时用的是一样一样的。\n",
    "关于数据集的信息和加载方式可以看我的《第一个人工智能程序》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "m_train = train_x_orig.shape[0] # 训练样本的数量\n",
    "m_test = test_x_orig.shape[0] # 测试样本的数量\n",
    "num_px = test_x_orig.shape[1] # 每张图片的宽/高\n",
    "\n",
    "# 为了方便后面进行矩阵运算，我们需要将样本数据进行扁平化和转置\n",
    "# 处理后的数组各维度的含义是（图片数据，样本数）\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n",
    "\n",
    "# 下面我们对特征数据进行了简单的标准化处理（除以255，使所有值都在[0，1]范围内）\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用上面的工具函数构建一个深度神经网络训练模型\n",
    "def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    X -- 数据集\n",
    "    Y -- 数据集标签\n",
    "    layers_dims -- 指示该深度神经网络用多少层，每层有多少个神经元\n",
    "    learning_rate -- 学习率\n",
    "    num_iterations -- 指示需要训练多少次\n",
    "    print_cost -- 指示是否需要在将训练过程中的成本信息打印出来，好知道训练的进度好坏。\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 返回训练好的参数。以后就可以用这些参数来识别新的陌生的图片\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "\n",
    "    # 初始化每层的参数w和b\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # 按照指示的次数来训练深度神经网络\n",
    "    for i in range(0, num_iterations):\n",
    "        # 进行前向传播\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # 计算成本\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # 进行反向传播\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # 更新参数，好用这些参数进行下一轮的前向传播\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # 打印出成本\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print (\"训练%i次后成本是: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # 画出成本曲线图\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练100次后成本是: 0.672053\n",
      "训练200次后成本是: 0.648263\n",
      "训练300次后成本是: 0.611507\n",
      "训练400次后成本是: 0.567047\n",
      "训练500次后成本是: 0.540138\n",
      "训练600次后成本是: 0.527930\n",
      "训练700次后成本是: 0.465477\n",
      "训练800次后成本是: 0.369126\n",
      "训练900次后成本是: 0.391747\n",
      "训练1000次后成本是: 0.315187\n",
      "训练1100次后成本是: 0.272700\n",
      "训练1200次后成本是: 0.237419\n",
      "训练1300次后成本是: 0.199601\n",
      "训练1400次后成本是: 0.189263\n",
      "训练1500次后成本是: 0.161189\n",
      "训练1600次后成本是: 0.148214\n",
      "训练1700次后成本是: 0.137775\n",
      "训练1800次后成本是: 0.129740\n",
      "训练1900次后成本是: 0.121225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24c3a1b7ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置好深度神经网络的层次信息——下面代表了一个4层的神经网络（12288是输入层），\n",
    "# 第一层有20个神经元，第二层有7个神经元。。。\n",
    "# 你也可以构建任意层任意神经元数量的神经网络，只需要更改下面这个数组就可以了\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# 根据上面的层次信息来构建一个深度神经网络，并且用之前加载的数据集来训练这个神经网络，得出训练后的参数\n",
    "parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们再编写一个预测函数，来用上面训练得到的参数来进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):   \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # 进行一次前向传播，得到预测结果\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "   \n",
    "    # 将预测结果转化成0和1的形式，即大于0.5的就是1，否则就是0\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.9808612440191385\n"
     ]
    }
   ],
   "source": [
    "# 对训练数据集进行预测\n",
    "pred_train = predict(train_x,parameters)\n",
    "print(\"预测准确率是: \"  + str(np.sum((pred_train == train_y) / train_x.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 对测试数据集进行预测\n",
    "pred_test = predict(test_x,parameters)\n",
    "print(\"预测准确率是: \"  + str(np.sum((pred_test == test_y) / test_x.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，比我们之前的的单神经元网络的0.7提升了0.1。不要小看这0.1，这是很难提升的。别说是0.1，如果全世界都只能做到0.9，如果你能提升0.05，那么你就享誉全球啦！\n",
    "\n",
    "当然，0.8还不是我们的极限，后面的文章我会带领大家继续提升它。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
